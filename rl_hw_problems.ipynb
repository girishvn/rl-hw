{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O0fFgFRGksH6"
   },
   "source": [
    "## Installation\n",
    "\n",
    "To run this you need several packages. First of all, you need anaconda, which you most likely already have if you're viewing this through jupyter. If not then check the readme on the class page.\n",
    "\n",
    "System requirements: This should work on all operating systems (Linux, Mac, and Windows). However, several of the environments in the OpenAI-gym require additional simulators which don't aren't easy to get on Windows. In any case, it is strongly recommended that you use Linux, although you should be ok with Mac. (HINT: if you're on Windows check out the Windows Subsystem for Linux (WSL), although it'll make visualizing your policies a little tricky).\n",
    "\n",
    "Then install the following packages (using conda or pip):\n",
    "\n",
    "- pytorch --> `conda install pytorch -c pytorch`\n",
    "- gym --> `pip install gym`\n",
    "- gym (the cool environments, doesnt work on Windows) --> `pip install gym[all]`\n",
    "(When install gym[all] don't worry if the mujoco installation doesn't work. That's a more advanced 3D physics simulator that has to be set up separately (see website). Anyway, we don't need it necessarily)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nomKMuz0lrWL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\anand\\anaconda3\\lib\\site-packages (1.8.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\anand\\anaconda3\\lib\\site-packages (from torch) (1.19.2)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\anand\\anaconda3\\lib\\site-packages (from torch) (3.7.4.3)\n",
      "Requirement already satisfied: gym==0.12.1 in c:\\users\\anand\\anaconda3\\lib\\site-packages (0.12.1)\n",
      "Requirement already satisfied: requests>=2.0 in c:\\users\\anand\\anaconda3\\lib\\site-packages (from gym==0.12.1) (2.24.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\anand\\anaconda3\\lib\\site-packages (from gym==0.12.1) (1.5.2)\n",
      "Requirement already satisfied: six in c:\\users\\anand\\anaconda3\\lib\\site-packages (from gym==0.12.1) (1.15.0)\n",
      "Requirement already satisfied: pyglet>=1.2.0 in c:\\users\\anand\\anaconda3\\lib\\site-packages (from gym==0.12.1) (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.10.4 in c:\\users\\anand\\anaconda3\\lib\\site-packages (from gym==0.12.1) (1.19.2)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\anand\\anaconda3\\lib\\site-packages (from requests>=2.0->gym==0.12.1) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\anand\\anaconda3\\lib\\site-packages (from requests>=2.0->gym==0.12.1) (2020.12.5)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\anand\\anaconda3\\lib\\site-packages (from requests>=2.0->gym==0.12.1) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\anand\\anaconda3\\lib\\site-packages (from requests>=2.0->gym==0.12.1) (2.10)\n",
      "Requirement already satisfied: future in c:\\users\\anand\\anaconda3\\lib\\site-packages (from pyglet>=1.2.0->gym==0.12.1) (0.18.2)\n"
     ]
    }
   ],
   "source": [
    "# If you're using colab, this will install the necessary packages!\n",
    "!pip install torch\n",
    "#!pip install gym\n",
    "!pip install gym==0.12.1\n",
    "#!wget https://pjreddie.com/media/files/rlhw_util.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NzmPUAD1ksH7"
   },
   "outputs": [],
   "source": [
    "import sys, os, time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.multiprocessing as mp\n",
    "from torch import distributions\n",
    "from torch.distributions import Categorical\n",
    "from itertools import islice\n",
    "\n",
    "import gym\n",
    "\n",
    "from IPython.core.debugger import set_trace # import break point\n",
    "import pickle as pkl\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WXry0cZLksH_"
   },
   "source": [
    "# Introduction\n",
    "Welcome to the RL playground. Your task is to implement the REINFORCE and A3C algorithm to solve various OpenAI-gym environments. If you are not familiar with OpenAI-gym, stop reading and visit https://gym.openai.com/envs/ to see all the tasks you can try to solve.\n",
    "\n",
    "In this homework, we will only look at tasks with a discrete (and small) action space. That being said, both algorithms can be modified slightly to work on tasks with continuous action spaces. For full credit you must fill in the code below so you achieve an average total reward per episode on the cartpole task (CartPole-v1) of at least 499 (for an episode length of 500) for both REINFORCE and A3C. Then you must apply your code to any one other environment in OpenAI-gym, and plot and compare the learning curves (average total reward per episode vs number of episodes trained on) between REINFORCE and A3C (where at least one of the algorithms shows significant improvement from initialization).\n",
    "\n",
    "Below there's an overview of what every iteration will look like, regardless of whether you want to train or evaluate your agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALLMODEL = True\n",
    "MEDMODEL = False\n",
    "LARGEMODEL = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IBY204WCksIA"
   },
   "outputs": [],
   "source": [
    "from rlhw_util import * # <-- look whats inside here - it could save you a lot of work!\n",
    "\n",
    "def run_iteration(mode, N, agent, gen, horizon=None, render=False):\n",
    "    train = mode == 'train' \n",
    "    if train: # if mode is 'train'\n",
    "        agent.train() # train the agent\n",
    "    else: # if mode is not 'train'\n",
    "        agent.eval() # evaluate agent\n",
    "\n",
    "    states, actions, rewards = zip(*[gen(horizon=horizon, render=render) for _ in range(N)])\n",
    "    \n",
    "    #if render and renderClose:\n",
    "    #    gen.env._env.close()     \n",
    "\n",
    "    loss = None # loss initilized as None\n",
    "    if train: # if mode is 'train'\n",
    "        loss = agent.learn(states, actions, rewards) # loss returned from the training the agent \n",
    "\n",
    "    reward = sum([r.sum() for r in rewards]) / N # average over all rewards \n",
    "\n",
    "    return reward, loss # return avg reward and loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FbOI_tXDksIC"
   },
   "source": [
    "## The Actor\n",
    "\n",
    "We need to learn a policy which, given some state, outputs a distribution over all possible actions. As this is deep RL, we'll use a deep neural network to turn the observed state into the requisite action distribution. From this action distribution we can choose what action to take using `get_action`. Pytorch, brilliant as it is, makes our task incredibly easy, as we can use the `torch.distributions.Categorical` class for sampling.\n",
    "\n",
    "You can experiment with all sorts of network architectures, but remember this is RL, not image classification on ImageNet, so you probably won't need a very deep network (HINT: look below at the state and action dimensionality to get a feel for the task)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QkW0tUvZksID"
   },
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Actor, self).__init__() # super used to inherit functionality of parent class (nn.Module), for subclass Actor, and instance self\n",
    "        \n",
    "        # TODO: Fill in the code to define your policy\n",
    "        \n",
    "        if SMALLMODEL:\n",
    "            Cin = state_dim # input layer dimension\n",
    "            Cout = action_dim # output layer dimension\n",
    "            hidden_dim1 = 256 # hidden layer dimension\n",
    "            \n",
    "            self.lin1 = nn.Linear(state_dim, hidden_dim1) # linear layer\n",
    "            self.lin2 = nn.Linear(hidden_dim1, action_dim) # linear layer\n",
    "            \n",
    "        elif MEDMODEL:\n",
    "            Cin = state_dim # input layer dimension\n",
    "            Cout = action_dim # output layer dimension\n",
    "            hidden_dim1 = 256 # hidden layer 1 dimension\n",
    "            hidden_dim2 = 512 # hidden layer 2 dimension\n",
    "            hidden_dim3 = 256 # hidden layer 3 dimension\n",
    "            \n",
    "            self.lin1 = nn.Linear(state_dim, hidden_dim1) # linear layer\n",
    "            self.lin2 = nn.Linear(hidden_dim1, hidden_dim2) # linear layer\n",
    "            self.lin3 = nn.Linear(hidden_dim2, hidden_dim3) # linear layer\n",
    "            self.lin4 = nn.Linear(hidden_dim3, action_dim) # linear layer\n",
    "            \n",
    "            \n",
    "        elif LARGEMODEL:\n",
    "            Cin = state_dim # input layer dimension\n",
    "            Cout = action_dim # output layer dimension\n",
    "            hidden_dim1 = 512 # hidden layer 1 dimension\n",
    "            hidden_dim2 = 512 # hidden layer 2 dimension\n",
    "            hidden_dim3 = 1024 # hidden layer 3 dimension\n",
    "            hidden_dim4 = 512 # hidden layer 4 dimension\n",
    "            hidden_dim5 = 512 # hidden layer 5 dimension\n",
    "            \n",
    "            self.lin1 = nn.Linear(state_dim, hidden_dim1) # linear layer\n",
    "            self.lin2 = nn.Linear(hidden_dim1, hidden_dim2) # linear layer\n",
    "            self.lin3 = nn.Linear(hidden_dim2, hidden_dim3) # linear layer\n",
    "            self.lin4 = nn.Linear(hidden_dim3, hidden_dim4) # linear layer\n",
    "            self.lin5 = nn.Linear(hidden_dim4, hidden_dim5) # linear layer\n",
    "            self.lin6 = nn.Linear(hidden_dim5, action_dim) # linear layer\n",
    "            \n",
    "        else:\n",
    "            return\n",
    "        \n",
    "        #raise NotImplementedError\n",
    "        \n",
    "    def forward(self, state):\n",
    "        \n",
    "        # TODO: Fill in the code to run a forward pass of your policy to get a distribution over actions (HINT: probabilities sum to 1)\n",
    "        \n",
    "        if SMALLMODEL:\n",
    "            state = F.relu(self.lin1(state)) # use relu transistion function\n",
    "            state = F.relu(self.lin2(state)) # use relu transistion function\n",
    "            return F.softmax(state, dim=-1) # sum probs to 1\n",
    "        \n",
    "        elif MEDMODEL:\n",
    "            state = F.relu(self.lin1(state)) # use relu transistion function\n",
    "            state = F.relu(self.lin2(state)) # use relu transistion function\n",
    "            state = F.relu(self.lin3(state)) # use relu transistion function\n",
    "            state = F.relu(self.lin4(state)) # use relu transistion function\n",
    "            return F.softmax(state, dim=-1) # sum probs to 1\n",
    "        \n",
    "        elif LARGEMODEL:\n",
    "            state = F.relu(self.lin1(state)) # use relu transistion function\n",
    "            state = F.relu(self.lin2(state)) # use relu transistion function\n",
    "            state = F.relu(self.lin3(state)) # use relu transistion function\n",
    "            state = F.relu(self.lin4(state)) # use relu transistion function\n",
    "            state = F.relu(self.lin5(state)) # use relu transistion function\n",
    "            state = F.relu(self.lin6(state)) # use relu transistion function\n",
    "            return F.softmax(state, dim=-1) # sum probs to 1\n",
    "            \n",
    "        else:\n",
    "            return\n",
    "            \n",
    "        #raise NotImplementedError\n",
    "\n",
    "    def get_policy(self, state):\n",
    "        return Categorical(self(state))\n",
    "\n",
    "    def get_action(self, state, greedy=None):\n",
    "        if greedy is None:\n",
    "            greedy = not self.training\n",
    "\n",
    "        policy = self.get_policy(state)\n",
    "        return MLE(policy) if greedy else policy.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5NYbREzFksIF"
   },
   "source": [
    "## The REINFORCE Agent\n",
    "\n",
    "The Actor defines our policy, but we also have to define how and when we'll be updating our policy, which brings us to the agent. The agent will house the policy (an `Actor`), and can then be used to generate rollouts (using `forward()`) or update the policy given a list of rollouts (using `learn()`).\n",
    "\n",
    "The REINFORCE algorithm naively uses the returns directly to weight the gradients, however this makes the variance in the policy gradient estimation very large. As a result, we will use a baseline which is a linear model which takes in a state and outputs the return (sounds like a value function, right?). Except we're not going to train our baseline using gradient descent, instead we'll just solve the linear system analytically in every iteration, and use the solution in the next iteration. Don't worry about training/updating the baseline, but you do have to use it in the right way. (Optional experiment: try removing the baseline and see how performance changes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nQ7mlwvLksIG"
   },
   "outputs": [],
   "source": [
    "class REINFORCE(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, discount=0.97, lr=1e-3, weight_decay=1e-4):\n",
    "        super(REINFORCE, self).__init__()\n",
    "        self.actor = Actor(state_dim, action_dim)\n",
    "        \n",
    "        self.baseline = nn.Linear(state_dim, 1)\n",
    "        \n",
    "        # TODO: create an optimizer for the parameters of your actor (HINT: use the passed in lr and weight_decay args)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr, weight_decay=weight_decay) # create optimized for actor\n",
    "\n",
    "        #raise NotImplementedError\n",
    "    \n",
    "        self.discount = discount\n",
    "        \n",
    "    def forward(self, state):\n",
    "        return self.actor.get_action(state)\n",
    "    \n",
    "    def learn(self, states, actions, rewards):\n",
    "        '''\n",
    "        Takes in three arguments each of which is a list with equal length. Each element in the list is a \n",
    "        pytorch tensor with 1 row for every step in the episode, and the columns are state_dim, action_dim, \n",
    "        and 1, respectively.\n",
    "        '''\n",
    "        \n",
    "        # TODO: implement the REINFORCE algorithm (HINT: check the slides/papers)\n",
    "        \n",
    "        returns = [compute_returns(rs, discount=self.discount) for rs in rewards]\n",
    "        \n",
    "        states, actions, returns = torch.cat(states), torch.cat(actions), torch.cat(returns)\n",
    "        \n",
    "        self.optimizer.zero_grad() # https://pytorch.org/docs/stable/optim.html\n",
    "        \n",
    "        lossSum = 0\n",
    "        for i in range(len(states)): # iterate through state, action, reward for single episode \n",
    "            s = states[i] # get state\n",
    "            a = actions[i] # get action\n",
    "            r = returns[i] # get associated reward\n",
    "\n",
    "            pi = self.actor.get_policy(s) # get the policy pi\n",
    "\n",
    "            #loss = -pi.log_prob(a) * r # https://pytorch.org/docs/stable/distributions.html\n",
    "            loss = pi.log_prob(a) * r # https://pytorch.org/docs/stable/distributions.html\n",
    "\n",
    "            lossSum += loss\n",
    "            \n",
    "        lossSum.backward()\n",
    "        self.optimizer.step()\n",
    "            \n",
    "        #raise NotImplementedError\n",
    "        \n",
    "        error = F.mse_loss(self.baseline(states).squeeze(), returns).detach()\n",
    "        solve(states, returns, out=self.baseline)\n",
    "        #error = F.mse_loss(self.baseline(states).squeeze(), returns).detach()\n",
    "        \n",
    "        return error.item() # Returns a rough estimate of the error in the baseline (dont worry about this too much)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ogkj6xjeksII"
   },
   "source": [
    "## The Critic\n",
    "\n",
    "Now we can introduce a critic, which is essentially a value function to estimate the expected discounted reward of a state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uGYwOY6AksIJ"
   },
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        \n",
    "        # TODO: define your value function network\n",
    "        Cin = state_dim # input layer dimension\n",
    "        Cout = 1 # output layer dimension - 1 for single value\n",
    "        hidden_dim1 = 256 # hidden layer dimension\n",
    "\n",
    "        self.lin1 = nn.Linear(Cin, hidden_dim1) # linear layer\n",
    "        self.lin2 = nn.Linear(hidden_dim1, Cout) # linear layer\n",
    "\n",
    "        #raise NotImplementedError\n",
    "\n",
    "    def forward(self, state):\n",
    "        \n",
    "        # TODO: apply your value function network to get a value given this batch of states\n",
    "        state = F.relu(self.lin1(state)) # use relu transistion function\n",
    "        state = F.relu(self.lin2(state)) # use relu transistion function\n",
    "        return F.softmax(state, dim=-1) # sum probs to 1\n",
    "        \n",
    "        #raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5rfkylePksIL"
   },
   "source": [
    "## The A3C Agent\n",
    "\n",
    "Now we can put the actor and critic together using the A3C algorithm. It turns out, the tasks in the gym are all so simple that there is essentially no gain in parallelization, so technically we're implementing A2C (no async), but the RL part is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QoZgaABTksIL"
   },
   "outputs": [],
   "source": [
    "class A3C(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, discount=0.97, lr=1e-3, weight_decay=1e-4):\n",
    "        super(A3C, self).__init__()\n",
    "        self.actor = Actor(state_dim, action_dim)\n",
    "        self.critic = Critic(state_dim)\n",
    "        self.baseline = nn.Linear(state_dim, 1)\n",
    "        \n",
    "        # TODO: create an optimizer for the parameters of your actor (HINT: use the passed in lr and weight_decay args)\n",
    "        # (HINT: the actor and critic have different objectives, so how many optimizers do you need?)\n",
    "        \n",
    "        # Create 2 optimizers, one for the actor, one for the critic (assume same lr and weight decay)\n",
    "        self.optimizerAct = optim.Adam(self.actor.parameters(), lr=lr, weight_decay=weight_decay) # create optimized for actor\n",
    "        self.optimizerCrit = optim.Adam(self.critic.parameters(), lr=lr, weight_decay=weight_decay) # create optimized for critic\n",
    "\n",
    "        #raise NotImplementedError\n",
    "    \n",
    "        self.discount = discount\n",
    "        \n",
    "    def forward(self, state):\n",
    "        return self.actor.get_action(state)\n",
    "    \n",
    "    def learn(self, states, actions, rewards):\n",
    "        \n",
    "        returns = [compute_returns(rs, discount=self.discount) for rs in rewards]\n",
    "        \n",
    "        states, actions, returns = torch.cat(states), torch.cat(actions), torch.cat(returns)\n",
    "        \n",
    "        # TODO: implement A3C (HINT: algorithm details found in A3C paper supplement) \n",
    "        # (HINT2: the algorithm is actually very similar to REINFORCE, the only difference is now we have a critic, what might that do?)\n",
    "        \n",
    "        self.optimizerAct.zero_grad() # https://pytorch.org/docs/stable/optim.html\n",
    "        self.optimizerCrit.zero_grad()\n",
    "        \n",
    "        lossSum = 0.0\n",
    "        for i in range(len(states)): # iterate through state, action, reward for single episode \n",
    "            s = states[i] # get state\n",
    "            a = actions[i] # get action\n",
    "            r = returns[i] # get associated reward\n",
    "            \n",
    "            # We used the pseudocode from the paper (page 14): https://arxiv.org/pdf/1602.01783.pdf\n",
    "            V = self.critic(s)\n",
    "            R = r + self.discount * V # check terminal state case??????????\n",
    "            pi = self.actor.get_policy(s) # get the policy pi\n",
    "            actLoss = -pi.log_prob(a) * (R - V) # https://pytorch.org/docs/stable/distributions.html\n",
    "            critLoss = -(R - V) ** 2\n",
    "            lossSum += actLoss + critLoss\n",
    "            \n",
    "        lossSum.backward()\n",
    "        self.optimizerAct.step()\n",
    "        self.optimizerCrit.step()\n",
    "        \n",
    "        error = F.mse_loss(self.baseline(states).squeeze(), returns).detach() # adapted from given code for actor \n",
    "        solve(states, returns, out=self.baseline)\n",
    "        #error = F.mse_loss(self.baseline(states).squeeze(), returns).detach()\n",
    "        \n",
    "        return error.item() # Returns a rough estimate of the error in the baseline (dont worry about this too much)\n",
    "        \n",
    "        #raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hq-zQWzAksIO"
   },
   "source": [
    "## Part 1: Balancing a pole with a cart\n",
    "\n",
    "First, we'll test both algorithms on a very simple toy system: the cartpole. Eventhough it's very low dimensional (state=4, action=2), this task is nontrival because it is underactuated. Nevertheless after a few thousand episodes our policy shouldn't have a problem! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RGBdP7leksIP"
   },
   "outputs": [],
   "source": [
    "# Optimization hyperparameters\n",
    "lr = 1e-3\n",
    "weight_decay = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8vAi8sXuksIS"
   },
   "outputs": [],
   "source": [
    "env_name = 'CartPole-v1' \n",
    "#env_name = 'LunarLander-v2'\n",
    "#env_name = 'Acrobot-v1'\n",
    "\n",
    "e = Pytorch_Gym_Env(env_name)\n",
    "state_dim = e.observation_space.shape[0]\n",
    "action_dim = e.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "4\n",
      "Box(4,)\n",
      "[4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\n",
      "Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "# Debug Cell\n",
    "\n",
    "print(action_dim)\n",
    "print(state_dim)\n",
    "print(e.observation_space)\n",
    "print(e.observation_space.high)\n",
    "print(e.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Meq1LT8NksIV",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A3C(\n",
      "  (actor): Actor(\n",
      "    (lin1): Linear(in_features=4, out_features=256, bias=True)\n",
      "    (lin2): Linear(in_features=256, out_features=2, bias=True)\n",
      "  )\n",
      "  (critic): Critic(\n",
      "    (lin1): Linear(in_features=4, out_features=256, bias=True)\n",
      "    (lin2): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      "  (baseline): Linear(in_features=4, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Choose what agent to use\n",
    "#agent = REINFORCE(state_dim, action_dim, lr=lr, weight_decay=weight_decay)\n",
    "agent = A3C(state_dim, action_dim, lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "total_episodes = 0\n",
    "print(agent) # Let's take a look at what we're working with..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WUl_VOypksIX"
   },
   "outputs": [],
   "source": [
    "# Create a \n",
    "gen = Generator(e, agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ja001im9ksIZ"
   },
   "source": [
    "### Let's do this!!\n",
    "\n",
    "Below is the loop to train and evaluate your agent. You can play around with the number of iterations to run, and the number of rollouts per iteration. \n",
    "\n",
    "You can rerun this cell multiple times to keep training your model for more episodes. In any case, it shouldn't take more than 30 min to an 1 hour to train. (training never took me more than 5 min). HINT: Keep an eye on the eval_reward, it'll be pretty noisy, but if that should be slowly increasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists for plottings model training\n",
    "\n",
    "# REINFORCE Files \n",
    "#file_name = 'data/cartpoleREINFORCE.pkl'\n",
    "#file_name = 'data/lunarlanderREINFORCE.pkl'\n",
    "#file_name = 'data/acrobatREINFORCE.pkl'\n",
    "\n",
    "# A3C Files\n",
    "file_name = 'data/cartpoleA3C.pkl'\n",
    "#file_name = 'data/lunarlander_A3C.pkl'\n",
    "#file_name = 'data/acrobatlander_A3C.pkl'\n",
    "\n",
    "\n",
    "# Lists to store plotting information\n",
    "episodeList = []\n",
    "totalRewardList = []\n",
    "trainLossList = []\n",
    "evalRewardList = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8oT0vzayksIa",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep:1010: reward=24.000, loss=37.787, eval=118.900\n",
      "Ep:1020: reward=32.000, loss=40.742, eval=245.500\n",
      "Ep:1030: reward=37.000, loss=111.563, eval=236.900\n",
      "Ep:1040: reward=23.800, loss=154.022, eval=214.800\n",
      "Ep:1050: reward=30.800, loss=126.136, eval=180.400\n",
      "Ep:1060: reward=18.000, loss=33.650, eval=210.300\n",
      "Ep:1070: reward=23.300, loss=49.865, eval=176.100\n",
      "Ep:1080: reward=20.300, loss=26.368, eval=284.800\n",
      "Ep:1090: reward=41.100, loss=86.481, eval=230.000\n",
      "Ep:1100: reward=23.400, loss=43.785, eval=196.100\n",
      "Ep:1110: reward=29.800, loss=51.320, eval=114.400\n",
      "Ep:1120: reward=24.400, loss=65.232, eval=171.500\n",
      "Ep:1130: reward=32.600, loss=140.103, eval=225.600\n",
      "Ep:1140: reward=31.500, loss=61.587, eval=295.900\n",
      "Ep:1150: reward=30.000, loss=51.124, eval=324.100\n",
      "Ep:1160: reward=18.400, loss=58.910, eval=425.000\n",
      "Ep:1170: reward=31.300, loss=148.251, eval=405.600\n",
      "Ep:1180: reward=32.800, loss=39.557, eval=340.300\n",
      "Ep:1190: reward=38.500, loss=76.025, eval=421.000\n",
      "Ep:1200: reward=33.800, loss=51.265, eval=459.400\n",
      "Ep:1210: reward=22.800, loss=42.615, eval=422.100\n",
      "Ep:1220: reward=26.400, loss=50.492, eval=478.300\n",
      "Ep:1230: reward=23.300, loss=86.653, eval=483.400\n",
      "Ep:1240: reward=43.100, loss=125.476, eval=429.200\n",
      "Ep:1250: reward=23.900, loss=76.806, eval=486.400\n",
      "Ep:1260: reward=33.300, loss=79.384, eval=468.300\n",
      "Ep:1270: reward=30.400, loss=59.151, eval=401.800\n",
      "Ep:1280: reward=30.900, loss=70.598, eval=478.600\n",
      "Ep:1290: reward=24.900, loss=31.218, eval=468.100\n",
      "Ep:1300: reward=29.500, loss=109.523, eval=500.000\n",
      "Success!!! You have solved cartpole task! Time for a bigger challenge!\n",
      "Ep:1310: reward=31.800, loss=46.556, eval=479.200\n",
      "Ep:1320: reward=29.900, loss=75.472, eval=478.700\n",
      "Ep:1330: reward=27.400, loss=65.934, eval=447.300\n",
      "Ep:1340: reward=32.800, loss=64.544, eval=479.800\n",
      "Ep:1350: reward=27.000, loss=60.147, eval=418.300\n",
      "Ep:1360: reward=36.600, loss=321.351, eval=382.400\n",
      "Ep:1370: reward=28.500, loss=59.069, eval=366.300\n",
      "Ep:1380: reward=28.800, loss=46.546, eval=303.100\n",
      "Ep:1390: reward=28.000, loss=54.713, eval=243.600\n",
      "Ep:1400: reward=31.200, loss=46.957, eval=206.600\n",
      "Ep:1410: reward=24.800, loss=48.945, eval=179.200\n",
      "Ep:1420: reward=34.400, loss=79.580, eval=152.700\n",
      "Ep:1430: reward=36.900, loss=77.245, eval=178.800\n",
      "Ep:1440: reward=24.900, loss=63.701, eval=197.900\n",
      "Ep:1450: reward=47.300, loss=264.462, eval=223.100\n",
      "Ep:1460: reward=31.000, loss=90.332, eval=189.500\n",
      "Ep:1470: reward=39.500, loss=103.435, eval=214.500\n",
      "Ep:1480: reward=32.800, loss=62.576, eval=204.900\n",
      "Ep:1490: reward=31.200, loss=44.978, eval=232.500\n",
      "Ep:1500: reward=46.600, loss=82.511, eval=206.600\n",
      "Ep:1510: reward=31.100, loss=39.367, eval=173.100\n",
      "Ep:1520: reward=31.100, loss=71.018, eval=205.600\n",
      "Ep:1530: reward=24.000, loss=18.091, eval=170.500\n",
      "Ep:1540: reward=26.400, loss=28.830, eval=180.300\n",
      "Ep:1550: reward=28.300, loss=51.825, eval=167.800\n",
      "Ep:1560: reward=33.200, loss=33.292, eval=179.400\n",
      "Ep:1570: reward=24.200, loss=42.857, eval=169.800\n",
      "Ep:1580: reward=26.000, loss=29.887, eval=258.000\n",
      "Ep:1590: reward=38.400, loss=262.621, eval=211.000\n",
      "Ep:1600: reward=33.900, loss=69.763, eval=378.700\n",
      "Ep:1610: reward=33.300, loss=21.876, eval=409.300\n",
      "Ep:1620: reward=39.200, loss=66.213, eval=400.300\n",
      "Ep:1630: reward=37.600, loss=42.484, eval=464.200\n",
      "Ep:1640: reward=41.700, loss=55.371, eval=500.000\n",
      "Success!!! You have solved cartpole task! Time for a bigger challenge!\n",
      "Ep:1650: reward=22.200, loss=49.080, eval=442.000\n",
      "Ep:1660: reward=46.300, loss=331.895, eval=469.000\n",
      "Ep:1670: reward=24.600, loss=74.620, eval=473.400\n",
      "Ep:1680: reward=36.000, loss=79.390, eval=448.100\n",
      "Ep:1690: reward=27.500, loss=83.977, eval=355.100\n",
      "Ep:1700: reward=29.700, loss=42.777, eval=281.700\n",
      "Ep:1710: reward=28.500, loss=29.246, eval=227.600\n",
      "Ep:1720: reward=42.000, loss=283.269, eval=222.000\n",
      "Ep:1730: reward=26.600, loss=117.816, eval=182.800\n",
      "Ep:1740: reward=25.800, loss=73.785, eval=167.100\n",
      "Ep:1750: reward=52.700, loss=156.927, eval=156.400\n",
      "Ep:1760: reward=23.900, loss=78.323, eval=159.100\n",
      "Ep:1770: reward=21.500, loss=33.121, eval=169.000\n",
      "Ep:1780: reward=32.300, loss=137.737, eval=169.700\n",
      "Ep:1790: reward=29.000, loss=52.801, eval=158.600\n",
      "Ep:1800: reward=32.700, loss=86.024, eval=154.900\n",
      "Ep:1810: reward=29.800, loss=165.814, eval=151.200\n",
      "Ep:1820: reward=28.500, loss=54.921, eval=155.800\n",
      "Ep:1830: reward=39.300, loss=237.066, eval=155.600\n",
      "Ep:1840: reward=35.800, loss=106.694, eval=146.300\n",
      "Ep:1850: reward=42.100, loss=89.343, eval=151.900\n",
      "Ep:1860: reward=24.900, loss=59.229, eval=159.700\n",
      "Ep:1870: reward=36.100, loss=113.102, eval=156.000\n",
      "Ep:1880: reward=49.200, loss=543.768, eval=148.500\n",
      "Ep:1890: reward=30.200, loss=55.632, eval=163.200\n",
      "Ep:1900: reward=37.400, loss=67.502, eval=174.400\n",
      "Ep:1910: reward=29.500, loss=26.272, eval=192.900\n",
      "Ep:1920: reward=29.700, loss=45.897, eval=177.400\n",
      "Ep:1930: reward=42.900, loss=103.431, eval=222.800\n",
      "Ep:1940: reward=38.800, loss=113.780, eval=240.800\n",
      "Ep:1950: reward=36.700, loss=56.308, eval=225.700\n",
      "Ep:1960: reward=54.300, loss=104.034, eval=244.400\n",
      "Ep:1970: reward=32.000, loss=70.541, eval=304.000\n",
      "Ep:1980: reward=27.400, loss=41.177, eval=268.100\n",
      "Ep:1990: reward=35.600, loss=63.915, eval=330.500\n",
      "Ep:2000: reward=30.900, loss=47.329, eval=351.700\n",
      "Done Training\n",
      "Saving Metrics to Plot\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/cartpoleA3C.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-e0e995ed5262>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;31m# dump values to pickle file for storage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpklOutput\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m     \u001b[0mpkl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mplotDict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpklOutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpkl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHIGHEST_PROTOCOL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/cartpoleA3C.pkl'"
     ]
    }
   ],
   "source": [
    "num_iter = 100\n",
    "#num_iter = 500 # for longer training\n",
    "num_train = 10\n",
    "num_eval = 10 # dont change this\n",
    "\n",
    "for itr in range(num_iter):\n",
    "    #agent.model.epsilon = epsilon * epsilon_decay ** (total_episodes / epsilon_decay_episodes)\n",
    "    #print('** Iteration {}/{} **'.format(itr+1, num_iter))\n",
    "    train_reward, train_loss = run_iteration('train', num_train, agent, gen)\n",
    "    #train_reward, train_loss = run_iteration('train', num_train, agent, gen, render=True)\n",
    "    eval_reward, _ = run_iteration('eval', num_eval, agent, gen)\n",
    "    total_episodes += num_train\n",
    "    print('Ep:{}: reward={:.3f}, loss={:.3f}, eval={:.3f}'.format(total_episodes, train_reward, train_loss, eval_reward))\n",
    "    \n",
    "    # add values to plotting lists\n",
    "    episodeList.append(total_episodes)\n",
    "    totalRewardList.append(train_reward)\n",
    "    trainLossList.append(train_loss)\n",
    "    evalRewardList.append(eval_reward)\n",
    "    \n",
    "    if eval_reward > 499 and env_name == 'CartPole-v1': # dont change this\n",
    "        print('Success!!! You have solved cartpole task! Time for a bigger challenge!')\n",
    "        #break\n",
    "        \n",
    "    if env_name == 'Acrobot-v1':\n",
    "        if len(episodeList) > 10:\n",
    "            if evalRewardList[-1] > -100:\n",
    "                if evalRewardList[-2] > -100: \n",
    "                    if evalRewardList[-3] > -100:\n",
    "                        if evalRewardList[-4] > -100:\n",
    "                            print('solved acrobat!')\n",
    "                            break\n",
    "\n",
    "\n",
    "print('Done Training')\n",
    "\n",
    "# save model\n",
    "# Saving Model Metrics\n",
    "print('Saving Metrics to Plot')\n",
    "\n",
    "# dictionary to hold all model data\n",
    "plotDict = {}\n",
    "plotDict['episodeList'] = episodeList\n",
    "plotDict['totalRewardList'] = totalRewardList\n",
    "plotDict['trainLossList'] = trainLossList\n",
    "plotDict['evalRewardList'] = evalRewardList\n",
    "\n",
    "# dump values to pickle file for storage\n",
    "with open(file_name, 'wb') as pklOutput:\n",
    "    pkl.dump(plotDict, pklOutput, pkl.HIGHEST_PROTOCOL)\n",
    "\n",
    "print('All Metrics Saved')\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sxlxf0YwksIc",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# You can visualize your policy at any time\n",
    "avgReward, avgLoss = run_iteration('eval', 5, agent, gen, render=True)\n",
    "print(avgReward, avgLoss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "0SrE9Kh0ksIe"
   },
   "source": [
    "### Analysis\n",
    "\n",
    "Plot the performance of each of your agents for the cartpole task and one additional task. When choosing a new environment, make sure is has a discrete action space. For each plot the x axis should show the total number of episodes the model was trained on, and the y axis shows the average total reward per episode.\n",
    "\n",
    "You can leave the plots as cell outputs below, or you can save them as images and submit them separately.\n",
    "\n",
    "### Deliverables\n",
    "- single plot showing both the REINFORCE algorithm's performance, and A3C's performance on the same plot for the cartpole environment (CartPole-v1).\n",
    "- single plot showing both the REINFORCE algorithm's performance, and A3C's performance on the same plot for a second environment of your choice (suggested -> LunarLander-v2, it's a little tricky but watching the agent fly spaceships is very entertaining!).\n",
    "- in every case you models have to learn something for full credit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "90yijcV0ksIf",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "CARTPOLE = True\n",
    "LUNARLANDER = True\n",
    "ACROBAT = True\n",
    "\n",
    "if CARTPOLE:\n",
    "    with open('data/cartpoleREINFORCE_smallmodel.pkl', 'rb') as cartpoleREINFORCE_smallmodel:\n",
    "        cpRDict_sml = pkl.load(cartpoleREINFORCE_smallmodel)\n",
    "        cpREINFORCE_sml_episode = cpRDict_sml['episodeList']\n",
    "        cpREINFORCE_sml_totalReward = cpRDict_sml['totalRewardList']\n",
    "        cpREINFORCE_sml_trainLoss = cpRDict_sml['trainLossList']\n",
    "        cpREINFORCE_sml_evalReward = cpRDict_sml['evalRewardList']\n",
    "        \n",
    "    with open('data/cartpoleREINFORCE_medmodel.pkl', 'rb') as cartpoleREINFORCE_mediummodel:\n",
    "        cpRDict_med = pkl.load(cartpoleREINFORCE_mediummodel)\n",
    "        cpREINFORCE_med_episode = cpRDict_med['episodeList']\n",
    "        cpREINFORCE_med_totalReward = cpRDict_med['totalRewardList']\n",
    "        cpREINFORCE_med_trainLoss = cpRDict_med['trainLossList']\n",
    "        cpREINFORCE_med_evalReward = cpRDict_med['evalRewardList']\n",
    "        \n",
    "    with open('data/cartpoleREINFORCE_largemodel.pkl', 'rb') as cartpoleREINFORCE_largemodel:\n",
    "        cpRDict_lrg = pkl.load(cartpoleREINFORCE_largemodel)\n",
    "        cpREINFORCE_lrg_episode = cpRDict_lrg['episodeList']\n",
    "        cpREINFORCE_lrg_totalReward = cpRDict_lrg['totalRewardList']\n",
    "        cpREINFORCE_lrg_trainLoss = cpRDict_lrg['trainLossList']\n",
    "        cpREINFORCE_lrg_evalReward = cpRDict_lrg['evalRewardList']\n",
    "    \n",
    "    plt.figure(figsize=(15,20))\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.subplot(3,1,1)\n",
    "    plt.plot(cpREINFORCE_sml_episode, cpREINFORCE_sml_totalReward,'r', label='Small Model')\n",
    "    plt.plot(cpREINFORCE_med_episode, cpREINFORCE_med_totalReward,'g', label='Medium Model')\n",
    "    plt.plot(cpREINFORCE_lrg_episode, cpREINFORCE_lrg_totalReward,'b', label='Large Model')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Avg Reward')\n",
    "    plt.title('Cartpole - Avg Reward vs Training Episodes')\n",
    "    \n",
    "    plt.subplot(3,1,2)\n",
    "    plt.plot(cpREINFORCE_sml_episode, cpREINFORCE_sml_trainLoss,'r', label='Small Model')\n",
    "    plt.plot(cpREINFORCE_med_episode, cpREINFORCE_med_trainLoss,'g', label='Medium Model')\n",
    "    plt.plot(cpREINFORCE_lrg_episode, cpREINFORCE_lrg_trainLoss,'b', label='Large Model')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Train Loss')\n",
    "    plt.title('Cartpole - Training Loss vs Training Episodes')\n",
    "    \n",
    "    plt.subplot(3,1,3)\n",
    "    plt.plot(cpREINFORCE_sml_episode, cpREINFORCE_sml_evalReward,'r', label='Small Model')\n",
    "    plt.plot(cpREINFORCE_med_episode, cpREINFORCE_med_evalReward,'g', label='Medium Model')\n",
    "    plt.plot(cpREINFORCE_lrg_episode, cpREINFORCE_lrg_evalReward,'b', label='Large Model')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Eval Reward')\n",
    "    plt.title('Cartpole - Eval Reward vs Training Episodes')\n",
    "\n",
    "    \n",
    "    \n",
    "if LUNARLANDER:\n",
    "    with open('data/lunarlanderREINFORCE_smallmodel.pkl', 'rb') as lunarlanderREINFORCE_smallmodel:\n",
    "        llRDict_sml = pkl.load(lunarlanderREINFORCE_smallmodel)\n",
    "        llREINFORCE_sml_episode = llRDict_sml['episodeList']\n",
    "        llREINFORCE_sml_totalReward = llRDict_sml['totalRewardList']\n",
    "        llREINFORCE_sml_trainLoss = llRDict_sml['trainLossList']\n",
    "        llREINFORCE_sml_evalReward = llRDict_sml['evalRewardList']\n",
    "        \n",
    "    with open('data/lunarlanderREINFORCE_medmodel.pkl', 'rb') as lunarlanderREINFORCE_mediummodel:\n",
    "        llRDict_med = pkl.load(lunarlanderREINFORCE_mediummodel)\n",
    "        llREINFORCE_med_episode = llRDict_med['episodeList']\n",
    "        llREINFORCE_med_totalReward = llRDict_med['totalRewardList']\n",
    "        llREINFORCE_med_trainLoss = llRDict_med['trainLossList']\n",
    "        llREINFORCE_med_evalReward = llRDict_med['evalRewardList']\n",
    "        \n",
    "    with open('data/lunarlanderREINFORCE_largemodel.pkl', 'rb') as lunarlanderREINFORCE_largemodel:\n",
    "        llRDict_lrg = pkl.load(lunarlanderREINFORCE_largemodel)\n",
    "        llREINFORCE_lrg_episode = llRDict_lrg['episodeList']\n",
    "        llREINFORCE_lrg_totalReward = llRDict_lrg['totalRewardList']\n",
    "        llREINFORCE_lrg_trainLoss = llRDict_lrg['trainLossList']\n",
    "        llREINFORCE_lrg_evalReward = llRDict_lrg['evalRewardList']\n",
    " \n",
    "    plt.figure(figsize=(15,20))\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.subplot(3,1,1)\n",
    "    plt.plot(llREINFORCE_sml_episode, llREINFORCE_sml_totalReward,'r', label='Small Model')\n",
    "    plt.plot(llREINFORCE_med_episode, llREINFORCE_med_totalReward,'g', label='Medium Model')\n",
    "    plt.plot(llREINFORCE_lrg_episode, llREINFORCE_lrg_totalReward,'b', label='Large Model')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Avg Reward')\n",
    "    plt.title('Lunar Lander - Avg Reward vs Training Episodes')\n",
    "    \n",
    "    plt.subplot(3,1,2)\n",
    "    plt.plot(llREINFORCE_sml_episode, llREINFORCE_sml_trainLoss,'r', label='Small Model')\n",
    "    plt.plot(llREINFORCE_med_episode, llREINFORCE_med_trainLoss,'g', label='Medium Model')\n",
    "    plt.plot(llREINFORCE_lrg_episode, llREINFORCE_lrg_trainLoss,'b', label='Large Model')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Train Loss')\n",
    "    plt.title('Lunar Lander - Training Loss vs Training Episodes')\n",
    "    \n",
    "    plt.subplot(3,1,3)\n",
    "    plt.plot(llREINFORCE_sml_episode, llREINFORCE_sml_evalReward,'r', label='Small Model')\n",
    "    plt.plot(llREINFORCE_med_episode, llREINFORCE_med_evalReward,'g', label='Medium Model')\n",
    "    plt.plot(llREINFORCE_lrg_episode, llREINFORCE_lrg_evalReward,'b', label='Large Model')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Eval Reward')\n",
    "    plt.title('Lunar Lander - Eval Reward vs Training Episodes')\n",
    "  \n",
    "    \n",
    "    \n",
    "if ACROBAT:\n",
    "    with open('data/acrobatREINFORCE_smallmodel.pkl', 'rb') as acrobatREINFORCE_smallmodel:\n",
    "        abRDict_sml = pkl.load(acrobatREINFORCE_smallmodel)\n",
    "        abREINFORCE_sml_episode = abRDict_sml['episodeList']\n",
    "        abREINFORCE_sml_totalReward = abRDict_sml['totalRewardList']\n",
    "        abREINFORCE_sml_trainLoss = abRDict_sml['trainLossList']\n",
    "        abREINFORCE_sml_evalReward = abRDict_sml['evalRewardList']\n",
    "        \n",
    "    with open('data/acrobatREINFORCE_medmodel.pkl', 'rb') as acrobatREINFORCE_mediummodel:\n",
    "        abRDict_med = pkl.load(acrobatREINFORCE_mediummodel)\n",
    "        abREINFORCE_med_episode = abRDict_med['episodeList']\n",
    "        abREINFORCE_med_totalReward = abRDict_med['totalRewardList']\n",
    "        abREINFORCE_med_trainLoss = abRDict_med['trainLossList']\n",
    "        abREINFORCE_med_evalReward = abRDict_med['evalRewardList']\n",
    "        \n",
    "    with open('data/acrobatREINFORCE_largemodel.pkl', 'rb') as acrobatREINFORCE_largemodel:\n",
    "        abRDict_lrg = pkl.load(acrobatREINFORCE_largemodel)\n",
    "        abREINFORCE_lrg_episode = abRDict_lrg['episodeList']\n",
    "        abREINFORCE_lrg_totalReward = abRDict_lrg['totalRewardList']\n",
    "        abREINFORCE_lrg_trainLoss = abRDict_lrg['trainLossList']\n",
    "        abREINFORCE_lrg_evalReward = abRDict_lrg['evalRewardList']\n",
    "\n",
    "    plt.figure(figsize=(15,20))\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.subplot(3,1,1)\n",
    "    plt.plot(abREINFORCE_sml_episode, abREINFORCE_sml_totalReward,'r', label='Small Model')\n",
    "    plt.plot(abREINFORCE_med_episode, abREINFORCE_med_totalReward,'g', label='Medium Model')\n",
    "    plt.plot(abREINFORCE_lrg_episode, abREINFORCE_lrg_totalReward,'b', label='Large Model')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Avg Reward')\n",
    "    plt.title('Acrobat - Avg Reward vs Training Episodes')\n",
    "    \n",
    "    plt.subplot(3,1,2)\n",
    "    plt.plot(abREINFORCE_sml_episode, abREINFORCE_sml_trainLoss,'r', label='Small Model')\n",
    "    plt.plot(abREINFORCE_med_episode, abREINFORCE_med_trainLoss,'g', label='Medium Model')\n",
    "    plt.plot(abREINFORCE_lrg_episode, abREINFORCE_lrg_trainLoss,'b', label='Large Model')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Train Loss')\n",
    "    plt.title('Acrobat - Training Loss vs Training Episodes')\n",
    "    \n",
    "    plt.subplot(3,1,3)\n",
    "    plt.plot(abREINFORCE_sml_episode, abREINFORCE_sml_evalReward,'r', label='Small Model')\n",
    "    plt.plot(abREINFORCE_med_episode, abREINFORCE_med_evalReward,'g', label='Medium Model')\n",
    "    plt.plot(abREINFORCE_lrg_episode, abREINFORCE_lrg_evalReward,'b', label='Large Model')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Eval Reward')\n",
    "    plt.title('Acrobat - Eval Reward vs Training Episodes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CARTPOLE:\n",
    "    with open('data/cartpoleREINFORCE_converge.pkl', 'rb') as cartpoleREINFORCE_conv:\n",
    "        cpRDict_conv = pkl.load(cartpoleREINFORCE_conv)\n",
    "        cpREINFORCE_conv_episode = cpRDict_conv['episodeList']\n",
    "        cpREINFORCE_conv_totalReward = cpRDict_conv['totalRewardList']\n",
    "        cpREINFORCE_conv_trainLoss = cpRDict_conv['trainLossList']\n",
    "        cpREINFORCE_conv_evalReward = cpRDict_conv['evalRewardList']\n",
    "    \n",
    "    plt.figure(figsize=(15,20))\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.subplot(3,1,1)\n",
    "    plt.plot(cpREINFORCE_conv_episode[0:200], cpREINFORCE_conv_totalReward[0:200])\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Avg Reward')\n",
    "    plt.title('Cartpole - Avg Reward vs Training Episodes')\n",
    "    \n",
    "    plt.subplot(3,1,2)\n",
    "    plt.plot(cpREINFORCE_conv_episode[0:200], cpREINFORCE_conv_trainLoss[0:200])\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Train Loss')\n",
    "    plt.title('Cartpole - Training Loss vs Training Episodes')\n",
    "    \n",
    "    plt.subplot(3,1,3)\n",
    "    plt.plot(cpREINFORCE_conv_episode[0:200], cpREINFORCE_conv_evalReward[0:200])\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Eval Reward')\n",
    "    plt.title('Cartpole - Eval Reward vs Training Episodes')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ACROBAT:\n",
    "    with open('data/acrobatREINFORCE_conv.pkl', 'rb') as acrobatREINFORCE_conv:\n",
    "        abRDict_conv = pkl.load(acrobatREINFORCE_conv)\n",
    "        abREINFORCE_conv_episode = abRDict_conv['episodeList']\n",
    "        abREINFORCE_conv_totalReward = abRDict_conv['totalRewardList']\n",
    "        abREINFORCE_conv_trainLoss = abRDict_conv['trainLossList']\n",
    "        abREINFORCE_conv_evalReward = abRDict_conv['evalRewardList']\n",
    "    \n",
    "    plt.figure(figsize=(15,20))\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.subplot(3,1,1)\n",
    "    plt.plot(cpREINFORCE_conv_episode[0:200], cpREINFORCE_conv_totalReward[0:200])\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Avg Reward')\n",
    "    plt.title('Cartpole - Avg Reward vs Training Episodes')\n",
    "    \n",
    "    plt.subplot(3,1,2)\n",
    "    plt.plot(cpREINFORCE_conv_episode[0:200], cpREINFORCE_conv_trainLoss[0:200])\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Train Loss')\n",
    "    plt.title('Cartpole - Training Loss vs Training Episodes')\n",
    "    \n",
    "    plt.subplot(3,1,3)\n",
    "    plt.plot(cpREINFORCE_conv_episode[0:200], cpREINFORCE_conv_evalReward[0:200])\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Eval Reward')\n",
    "    plt.title('Cartpole - Eval Reward vs Training Episodes')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "rl-hw-problems.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
